# ç¯å¢ƒå˜é‡é…ç½®æŒ‡å—

æœ¬é¡¹ç›®ä½¿ç”¨ `.env` æ–‡ä»¶ç»Ÿä¸€ç®¡ç†å…¨å±€å˜é‡é…ç½®ã€‚

## ğŸ“‹ é…ç½®é¡¹è¯´æ˜

### LLM API é…ç½®

æ”¯æŒä»»ä½• OpenAI-compatible APIï¼ˆOpenAIã€Azure OpenAIã€æœ¬åœ°éƒ¨ç½²ç­‰ï¼‰

```env
# API Keyï¼ˆå¿…éœ€ï¼‰
LLM_API_KEY=sk-your-api-key-here

# Base URLï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º OpenAI APIï¼‰
LLM_BASE_URL=https://api.openai.com/v1

# æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º gpt-3.5-turboï¼‰
LLM_MODEL=gpt-3.5-turbo
```

**å…¼å®¹æ¨¡å¼**ï¼š
- å¦‚æœè®¾ç½®äº† `OPENAI_API_KEY`ï¼Œä¼šè¢« `LLM_API_KEY` è¦†ç›–
- è¿™æ ·å¯ä»¥å…¼å®¹æ—§çš„ç¯å¢ƒå˜é‡é…ç½®

### ASR/TTS æ¨¡å‹é…ç½®

```env
# ASR æ¨¡å‹
GLM_ASR_MODEL=THUDM/glm-4-voice-9b

# TTS æ¨¡å‹
QWEN_TTS_MODEL=Qwen/Qwen2.5-1.5B-Instruct
```

### æ¨ç†æœåŠ¡é…ç½®

```env
# ASR æœåŠ¡åœ°å€
ASR_HOST=127.0.0.1
ASR_PORT=8765

# TTS æœåŠ¡åœ°å€
TTS_HOST=127.0.0.1
TTS_PORT=8766
```

### CUDA é…ç½®

```env
# æŒ‡å®šä½¿ç”¨çš„ GPU è®¾å¤‡
CUDA_VISIBLE_DEVICES=0
```

---

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### 1. OpenAI å®˜æ–¹ API

```env
LLM_API_KEY=sk-xxx
LLM_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-3.5-turbo
```

### 2. Azure OpenAI

```env
LLM_API_KEY=your-azure-key
LLM_BASE_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment
LLM_MODEL=gpt-35-turbo
```

### 3. æœ¬åœ°éƒ¨ç½²ï¼ˆOllamaã€vLLM ç­‰ï¼‰

```env
LLM_API_KEY=dummy-key  # æŸäº›æœ¬åœ°æœåŠ¡ä¸éœ€è¦çœŸå® key
LLM_BASE_URL=http://localhost:11434/v1
LLM_MODEL=llama2
```

### 4. å›½å†… APIï¼ˆæ™ºè°±ã€æœˆä¹‹æš—é¢ç­‰ï¼‰

```env
# æ™ºè°± GLM
LLM_API_KEY=your-glm-key
LLM_BASE_URL=https://open.bigmodel.cn/api/paas/v4
LLM_MODEL=glm-4

# æœˆä¹‹æš—é¢ Kimi
LLM_API_KEY=your-kimi-key
LLM_BASE_URL=https://api.moonshot.cn/v1
LLM_MODEL=moonshot-v1-8k
```

---

## ğŸ”§ æŠ€æœ¯å®ç°

### Rust ç«¯ (Tauri)

ä½¿ç”¨ `dotenvy` crate åŠ è½½ .env æ–‡ä»¶ï¼š

```rust
// src-tauri/src/lib.rs
use dotenvy::dotenv;

pub fn run() {
    // åŠ è½½ .env æ–‡ä»¶
    if let Err(e) = dotenv() {
        eprintln!("Warning: Failed to load .env file: {}", e);
    }

    // åç»­ä»£ç ...
}
```

```rust
// src-tauri/src/llm/client.rs
use async_openai::config::OpenAIConfig;
use std::env;

impl LlmClient {
    pub fn new() -> Result<Self> {
        // è¯»å–ç¯å¢ƒå˜é‡
        let api_key = env::var("LLM_API_KEY")
            .or_else(|_| env::var("OPENAI_API_KEY"))?;

        let base_url = env::var("LLM_BASE_URL")
            .unwrap_or_else(|_| "https://api.openai.com/v1".to_string());

        let model = env::var("LLM_MODEL")
            .unwrap_or_else(|_| "gpt-3.5-turbo".to_string());

        // é…ç½® OpenAI å®¢æˆ·ç«¯
        let config = OpenAIConfig::default()
            .with_api_key(api_key)
            .with_api_base(base_url);

        let client = Client::with_config(config);

        Ok(Self { client, model })
    }
}
```

### Python ç«¯

ä½¿ç”¨ `python-dotenv` åŒ…åŠ è½½ .env æ–‡ä»¶ï¼š

```python
# inference/llm_client_example.py
import os
from dotenv import load_dotenv
from openai import OpenAI

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# è¯»å–ç¯å¢ƒå˜é‡
API_KEY = os.getenv("LLM_API_KEY") or os.getenv("OPENAI_API_KEY")
BASE_URL = os.getenv("LLM_BASE_URL", "https://api.openai.com/v1")
MODEL = os.getenv("LLM_MODEL", "gpt-3.5-turbo")

# åˆ›å»ºå®¢æˆ·ç«¯
client = OpenAI(
    api_key=API_KEY,
    base_url=BASE_URL
)

# å‘é€è¯·æ±‚
response = client.chat.completions.create(
    model=MODEL,
    messages=[{"role": "user", "content": "Hello"}],
    stream=True
)
```

---

## ğŸ“¦ ä¾èµ–å®‰è£…

### Rust

å·²åœ¨ `Cargo.toml` ä¸­æ·»åŠ ï¼š

```toml
dotenvy = "0.15"
async-openai = "0.14"
```

### Python

å·²åœ¨ `requirements-asr.txt` å’Œ `requirements-tts.txt` ä¸­æ·»åŠ ï¼š

```txt
python-dotenv>=1.0.0
openai>=1.0.0
```

å®‰è£…ä¾èµ–ï¼š

```bash
# ASR ç¯å¢ƒ
cd inference
venv\Scripts\activate
pip install -r requirements-asr.txt

# TTS ç¯å¢ƒ
cd inference
venv-tts\Scripts\activate
pip install -r requirements-tts.txt
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. .env æ–‡ä»¶ä½ç½®

- `.env` æ–‡ä»¶åº”æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼ˆ`F:\GitRepository\sisyphus\.env`ï¼‰
- Rust ä¼šè‡ªåŠ¨ä»å½“å‰å·¥ä½œç›®å½•æˆ–çˆ¶ç›®å½•æŸ¥æ‰¾ .env æ–‡ä»¶
- Python éœ€è¦ç¡®ä¿ `load_dotenv()` åœ¨è¯»å–ç¯å¢ƒå˜é‡ä¹‹å‰è°ƒç”¨

### 2. å®‰å…¨æ€§

- âš ï¸ **æ°¸è¿œä¸è¦æäº¤ .env æ–‡ä»¶åˆ° Gitï¼**
- `.env` å·²æ·»åŠ åˆ° `.gitignore`
- ä½¿ç”¨ `.env.example` ä½œä¸ºæ¨¡æ¿

### 3. ç¯å¢ƒå˜é‡ä¼˜å…ˆçº§

1. ç³»ç»Ÿç¯å¢ƒå˜é‡
2. .env æ–‡ä»¶ä¸­çš„å˜é‡
3. ä»£ç ä¸­çš„é»˜è®¤å€¼

### 4. OpenAI åŒ…ç‰ˆæœ¬

Python ç«¯ä½¿ç”¨ `openai>=1.0.0`ï¼ˆæ–°ç‰ˆ APIï¼‰ï¼š
- âœ… æ”¯æŒ OpenAI-compatible API
- âœ… æ— éœ€ `tiktoken` åŒ…
- âœ… ç®€åŒ–çš„ API æ¥å£

---

## ğŸ§ª æµ‹è¯•é…ç½®

### æµ‹è¯• Rust é…ç½®

```bash
cd src-tauri

# è®¾ç½®ç¯å¢ƒå˜é‡
$env:LLM_API_KEY = "your-key"
$env:LLM_BASE_URL = "https://api.openai.com/v1"
$env:LLM_MODEL = "gpt-3.5-turbo"

# ç¼–è¯‘æµ‹è¯•
cargo build
```

### æµ‹è¯• Python é…ç½®

```bash
cd inference

# æ¿€æ´»ç¯å¢ƒ
venv\Scripts\activate

# è¿è¡Œç¤ºä¾‹
python llm_client_example.py
```

---

## ğŸ“ å¸¸è§é—®é¢˜

### Q: å¦‚ä½•ä½¿ç”¨æœ¬åœ° LLMï¼Ÿ

A: è®¾ç½® `LLM_BASE_URL` æŒ‡å‘æœ¬åœ°æœåŠ¡ï¼Œä¾‹å¦‚ï¼š

```env
LLM_BASE_URL=http://localhost:11434/v1  # Ollama
LLM_MODEL=llama2
```

### Q: ä¸ºä»€ä¹ˆä¸ä½¿ç”¨ tiktokenï¼Ÿ

A:
- tiktoken ä»…ç”¨äº token è®¡æ•°ï¼Œå¯¹ LLM è°ƒç”¨ä¸æ˜¯å¿…éœ€çš„
- OpenAI-compatible API é€šå¸¸ä¸éœ€è¦æœ¬åœ° token è®¡æ•°
- å‡å°‘ä¾èµ–ï¼Œç®€åŒ–éƒ¨ç½²

### Q: å¦‚ä½•åˆ‡æ¢ä¸åŒçš„ API æä¾›å•†ï¼Ÿ

A: åªéœ€ä¿®æ”¹ `.env` ä¸­çš„ä¸‰ä¸ªé…ç½®é¡¹ï¼š

```env
LLM_API_KEY=new-key
LLM_BASE_URL=new-base-url
LLM_MODEL=new-model
```

é‡å¯åº”ç”¨å³å¯ç”Ÿæ•ˆã€‚

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs/api-reference)
- [async-openai Rust crate](https://docs.rs/async-openai/)
- [python-dotenv æ–‡æ¡£](https://pypi.org/project/python-dotenv/)
- [OpenAI Python SDK](https://github.com/openai/openai-python)
